{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a29e9d-6c7e-497e-a228-fb66bdb7d1e9",
   "metadata": {},
   "source": [
    "## Eigene Umgebung\n",
    "\n",
    "Autofahrer\n",
    "\n",
    "Soll die Geschwindigkeit immer zw. 45km/h und 55km/h halten\n",
    "\n",
    "Drei Actions: Beschleunigen, Bremsen, Nichts tun\n",
    "\n",
    "Während des Prozesses wird das Auto automatisch langsamer/schneller (Bergauf/Bergab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff759f99-4aec-4aca-8932-deb5a16951df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "from typing import Optional\n",
    "from gymnasium.spaces import Discrete, Box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbf090c-fb08-4354-a38b-83f0c41a98cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CarDriver(gym.Env):\n",
    "    def __init__(self):\n",
    "        self.action_space = Discrete(3)\n",
    "        self.observation_space = Box(0, 100, dtype=np.int8)\n",
    "        self.state = 50\n",
    "        self.driveLength = 60\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 1:\n",
    "            self.state += 3\n",
    "        elif action == 2:\n",
    "            self.state -= 3\n",
    "\n",
    "        if self.state >= 45 and self.state <= 55:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = 0\n",
    "\n",
    "        self.state += np.random.randint(3) - 1\n",
    "        self.driveLength -= 1\n",
    "\n",
    "        return np.array(self.state), reward, self.driveLength <= 0, False, {}\n",
    "    \n",
    "    def reset(self, *, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        self.state = 50\n",
    "        self.driveLength = 60\n",
    "        return (np.array(self.state), {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd3027d3-ab86-4e13-80a9-099c22dac7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ce3e028-cb5f-44c5-9193-54ed50dc0f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchgang: 0, Score: 8\n",
      "Durchgang: 1, Score: 16\n",
      "Durchgang: 2, Score: 16\n",
      "Durchgang: 3, Score: 39\n",
      "Durchgang: 4, Score: 19\n",
      "Durchgang: 5, Score: 2\n",
      "Durchgang: 6, Score: 15\n",
      "Durchgang: 7, Score: 8\n",
      "Durchgang: 8, Score: 6\n",
      "Durchgang: 9, Score: 16\n"
     ]
    }
   ],
   "source": [
    "durchgaenge = 10\n",
    "for d in range(durchgaenge):\n",
    "    start = env.reset()  # Am Anfang von jedem Durchgang wird die Umgebung zurückgesetzt\n",
    "    score = 0  # Summiert die Belohnung nach jedem Step auf\n",
    "    done = False\n",
    "\n",
    "    while not done:  # Führe die Umgebung aus, bis die step Methode True zurück gibt\n",
    "        action = env.action_space.sample()  # Wähle eine zufällige Action, und füge diese bei step ein\n",
    "        newState, reward, done, term, info = env.step(action)  # Führe die Umgebung einmal aus\n",
    "        score += reward\n",
    "    print(f\"Durchgang: {d}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3f2f5ef6-1ffc-47e6-b37d-4ace56cd02d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | 21.4     |\n",
      "| time/              |          |\n",
      "|    fps             | 795      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 21.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 458         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005709998 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.0155     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.11        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00149    |\n",
      "|    value_loss           | 16.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 23          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 427         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006441885 |\n",
      "|    clip_fraction        | 0.00786     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.022       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.2         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00177    |\n",
      "|    value_loss           | 22.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 23.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 419          |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 19           |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0109647345 |\n",
      "|    clip_fraction        | 0.0883       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | 0.0823       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.1         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00722     |\n",
      "|    value_loss           | 23.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 24.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 415          |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0054638903 |\n",
      "|    clip_fraction        | 0.0266       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | -0.132       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.3         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.00196     |\n",
      "|    value_loss           | 28.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 24.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 406         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011037885 |\n",
      "|    clip_fraction        | 0.00928     |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0345      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.4        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00301    |\n",
      "|    value_loss           | 28.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 25.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 389          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0035085331 |\n",
      "|    clip_fraction        | 0.00566      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.113        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 13.2         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00221     |\n",
      "|    value_loss           | 30.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 26.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 378          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 43           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0058573214 |\n",
      "|    clip_fraction        | 0.00503      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0838       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.4         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 29.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 27           |\n",
      "| time/                   |              |\n",
      "|    fps                  | 369          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 49           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0119284745 |\n",
      "|    clip_fraction        | 0.134        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.0712       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 18.7         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.0111      |\n",
      "|    value_loss           | 32.3         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 30.8         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 359          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 57           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0022305981 |\n",
      "|    clip_fraction        | 0.00327      |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.3         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000402    |\n",
      "|    value_loss           | 34.9         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x109481b7d40>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "env = CarDriver()\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=20_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15f7fb70-58d3-4a52-9741-8a9f38c3ab6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lk3\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\save_util.py:284: UserWarning: Path 'Models' does not exist. Will create it.\n",
      "  warnings.warn(f\"Path '{path.parent}' does not exist. Will create it.\")\n"
     ]
    }
   ],
   "source": [
    "model.save(\"Models/CarDriver\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4666df60-8f89-45b6-afcc-36d03c2f5187",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CarDriver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "407546f9-0f51-48eb-9978-ca20bf5313e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"Models/CarDriver\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75a3db0b-7bee-4b30-b978-e85faa251184",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:  \u001b[38;5;66;03m# Führe die Umgebung aus, bis die step Methode True zurück gibt\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m     state, reward, done, term, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# Führe die Umgebung einmal aus\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\base_class.py:556\u001b[0m, in \u001b[0;36mBaseAlgorithm.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    538\u001b[0m     observation: Union[np\u001b[38;5;241m.\u001b[39mndarray, Dict[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray]],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    541\u001b[0m     deterministic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    542\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[np\u001b[38;5;241m.\u001b[39mndarray, Optional[Tuple[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]]]:\n\u001b[0;32m    543\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    544\u001b[0m \u001b[38;5;124;03m    Get the policy action from an observation (and optional hidden state).\u001b[39;00m\n\u001b[0;32m    545\u001b[0m \u001b[38;5;124;03m    Includes sugar-coating to handle different observations (e.g. normalizing images).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    554\u001b[0m \u001b[38;5;124;03m        (used in recurrent policies)\u001b[39;00m\n\u001b[0;32m    555\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 556\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisode_start\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\policies.py:357\u001b[0m, in \u001b[0;36mBasePolicy.predict\u001b[1;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[38;5;66;03m# Check for common mistake that the user does not mix Gym/VecEnv API\u001b[39;00m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;66;03m# Tuple obs are not supported by SB3, so we can safely do that check\u001b[39;00m\n\u001b[0;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(observation) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(observation[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m--> 357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have passed a tuple to the predict() function instead of a Numpy array or a Dict. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    360\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvs `obs = vec_env.reset()` (SB3 VecEnv). \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    361\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSee related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    362\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mand documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    363\u001b[0m     )\n\u001b[0;32m    365\u001b[0m obs_tensor, vectorized_env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobs_to_tensor(observation)\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m th\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[1;31mValueError\u001b[0m: You have passed a tuple to the predict() function instead of a Numpy array or a Dict. You are probably mixing Gym API with SB3 VecEnv API: `obs, info = env.reset()` (Gym) vs `obs = vec_env.reset()` (SB3 VecEnv). See related issue https://github.com/DLR-RM/stable-baselines3/issues/1694 and documentation for more information: https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api"
     ]
    }
   ],
   "source": [
    "durchgaenge = 5\n",
    "for d in range(durchgaenge):\n",
    "    state = env.reset()  # Am Anfang von jedem Durchgang wird die Umgebung zurückgesetzt\n",
    "    score = 0  # Summiert die Belohnung nach jedem Step auf\n",
    "    done = False\n",
    "    \n",
    "    while not done:  # Führe die Umgebung aus, bis die step Methode True zurück gibt\n",
    "        if len(state) == 2:\n",
    "            action, _ = model.predict(state[0])\n",
    "        else:\n",
    "            action, _ = model.predict(state)\n",
    "        state, reward, done, term, info = env.step(action)  # Führe die Umgebung einmal aus\n",
    "        score += reward\n",
    "    print(f\"Durchgang: {d}, Score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f63fdb-60c3-4183-a9ce-b40b48a42e0c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
